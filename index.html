<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
</title>

    <link rel="shortcut icon" href="/static/app/global/favicon.ico">
    <link rel="apple-touch-icon" href="/static/app/global/logo.png">
    <!-- Bootstrap -->
    <link href="/static/app/_bower_components/bootstrap/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="/static/app/global/style.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

<link href="/static/pages/decision-trees-part-1/styles.css" rel="stylesheet">
<script type="text/javascript">
document.addEventListener("DOMContentLoaded", function(event) { 
  var dismissText = document.getElementById("dismiss-disclaimer")
  var removeDisclaimer

  removeDisclaimer = function() {
    var disclaimer = document.getElementById('resolution-disclaimer')
    disclaimer.remove()

    dismissText.removeEventListener("click", removeDisclaimer, false)
  }

  dismissText.addEventListener("click", removeDisclaimer, false)
});

</script>
<style>
</style>

  </head>
  <body>
    <div id="header">
      <div class="container">
        <div class="row">
          <div class="col-xs-11 col-sm-4">
            <a id="logo" class="hide-text" href="/">R2D3</a>
          </div>
        </div>
      </div>
    </div>
    
<div id="platform"></div>

<div class="container" id="main">
  <div class="row" id="intro">
    <div class="col-xs-8">
      
      <div id="set-up" class="tracking-section">
        <div id="recap">
          <!--<span><strong>Recap</strong>: In <a href="/visual-intro-to-machine-learning-part-1/" target="_blank">Part 1</a>, we created a model that distinguishes homes in San Francisco from those in New York. 
              While the model was 100% accurate on the test data, it performed less well on the training data. 
              The model was imperfect because it overfit the data.</span>-->
        </div>

        <div id="title-text">
          <span id="sub-title">A visual introduction<br /> to machine learning&mdash;Part II</span>
          <h1>Model Tuning and<br />the&nbsp;Bias-Variance&nbsp;Tradeoff</h1>
          <p>The goal of modeling is to approximate real-life situations by identifying and encoding patterns in data. Models make mistakes if those patterns are overly simple or overly complex. </p>  
          <p>In <a href="/visual-intro-to-machine-learning-part-1/" target="_blank">Part 1</a>, we created a model that distinguishes homes in San Francisco from those in New York. Now, we'll talk about tuning and the Bias-Variance tradeoff.</p>
        </div>
      </div>
    </div>
    <div class="col-xs-5">
      <div id="tuning">
        <div id="given-a-slice">
          <hr class="whitespace" style="height: 20vh;" />
          <h2>Model parameters</h2>
           <p>Models can be adjusted to change the way they fit the data. These 'settings' are called <strong>parameters</strong>. An example of a decision-tree parameter is the <em>minimum node size</em>, which regulates the creation of new splits. A node will not split if the number of data points it contains is below the minimum node size.</p>
          <hr class="whitespace" style="height: 20vh;" />
        </div>
        <div id="when-models-make-mistakes">
            <p>The tree from <a href="/visual-intro-to-machine-learning-part-1/" target="_blank">Part 1</a> had a minimum node size of one. It was very complex, had lots of splits, and overfit the data.
              To see why, <strong>let&rsquo;s revisit how the decision tree was trained.</strong>
            </p>

        </div>
        <hr class="whitespace" style="height: 35vh;" />
      </div>
    </div>
  </div>
  <div class="row" id="approximation">
    <div class="col-xs-5">
      <div id="bias">
        <div id="stump">
          <hr class="whitespace" style="height: 1vh;" />    
          <h2>Overly simple</h2>
          <p>
              The simplest version of a decision tree is called a <em>stump</em>. Comprised of a single split, stumps are comprised of a single rule, such as &ldquo;Every house whose elevation is above 34 feet is in San Francisco, and all others are in New York.&rdquo;
            </p>
          <hr class="whitespace" style="height: 20vh;" />
          <p>Stumps take a <em>binary</em> view to the world and ignore complexity and nuance in the training data. This black-and-white interpretation of the world is prone to errors due to <strong>bias</strong>.</p>
          <hr class="whitespace" style="height: 20vh;" />
          <p id="systematic-bias">
            A model with too much bias systematically ignores relevant details and is wrong in <em>consistent</em> ways. The stump to the right incorrectly classifies all lower-elevation homes in San Francisco.
          </p>
          <hr class="whitespace" style="height: 40vh;" />

          <p>To decrease the error due to bias, you can add additional splits to the tree. </p>
          <hr class="whitespace" style="height: 20vh;" />
        </div>
        <hr class="whitespace" style="height: 20vh;" />
        <p></p>
        <hr class="whitespace" style="height: 20vh;" />
      </div>
      <div id="variance">
        <h2>Overly complex</h2>
        <p>
          Additional splits allow the tree to take into account more complexity. You can add splits until a tree's leaf nodes contain only homes in either San Francisco or New York.
        </p>
        <hr class="whitespace" style="height: 45vh;" />
        <p>The question is, how does it perform on the test data?</p>
        <div id="test-set-classified">  
          <hr class="whitespace" style="height: 45vh;" />
          <p>As we observed in Part I, this <em>fully-grown</em> tree is not as accurate on test data. For example, with this tree the error rate is 12%.</p>
          </p>
          <p>This overly-complex tree suffers from errors due to <strong>variance</strong>.</p>
          <p>High-variance models make mistakes by overfitting to the idiosyncrasies of the training data. They tend to be wrong in <em>inconsistent</em> ways.</p>
        </div>
        <hr class="whitespace" style="height: 25vh;" />
        <p>To see exactly what's going on, let&rsquo;s switch back to the <strong>training set</strong> and 
        
        the creation of a single leaf node.</p>
        <hr class="whitespace" style="height: 25vh;" />
        <div id="isolate-sample">
          <div id="do-you-really-believe">
          <hr class="whitespace" style="height: 20vh;" />
            <h2>A tangible example of variance</h2>
            <p>This leaf node is the result of eight separate forks. Each fork divides the data set into smaller subsets, until the leaf node contains a single San Francisco home.</p>
            <div id="split-anchors-move">
              <hr class="whitespace" style="height: 30vh;" />
            </div>
            <p>To see how repeated splitting leads to variance errors, we will show the series of forks as distribution dot plots.
            </p>
          </div>
        </div>
        <div id="split-water-fall">        
          <hr class="whitespace" style="height: 30vh;" />
          <p>At each fork, the split point is set such that the resulting branches are as homogeneous as possible. </p>
          <div id="waterfall-parallax">
            <hr class="whitespace" style="height: 35vh;" />
            <p>
              Each split point is selected greedily, rather than taking into account what might be better later on. Splits earlier in the tree can have a cascading effect deeper in the tree. 
            </p>
            <hr class="whitespace" style="height: 35vh;" />
            <p>
              The deeper you go into the branch, the less the data is available to create splits. 
            </p>
            <hr class="whitespace" style="height: 35vh;" />
            <p>&hellip;At an extreme case (like this one), the final split is based on only two rather arbitrary data points. </p>
            <div id="waterfall-test-set">
              <hr class="whitespace" style="height: 35vh;" />
              <p>The arbitrariness of the split is reflected in model accuracy. If you put the test data through the tree, five homes satisfy the rules along this branch. The model thinks these homes should be in San Francisco.</p>
              <hr class="whitespace" style="height: 10vh;" />              
              <p>&hellip;but all five are in New York.</p>
              <hr class="whitespace" style="height: 30vh;" />
            </div>
          </div>
          <p>The final forks were made using very little data, so it's no surprise that the generalizations they make are incorrect. Patterns drawn from two homes are more likely to be flukes than anything real. 
            <span class="footnote-anchor-v2">
              <span>The fact that the node is 100% wrong is surprising but useful.</span>
            </span></p>
        </div>
      </div>
    </div>
  </div>
  <div class="row" id="more-trees" style="margin-top: -50vh; padding-top: 100vh;">
    <div class="col-xs-5">
      <div id="mini-trees">
        <h2>Flukes are normal</h2>        
        <p>This is not an isolated incident. For example, we could grow additional trees from random subsets of the training data.</p>
        <p>From the training set of 250 homes&hellip;</p>
        <div id="mini-tree-sampling">
          <hr class="whitespace" style="height: 35vh;" />
          <p>&hellip; let's draw four <strong>random samples</strong> of 200 homes each and grow trees based on them.</p>
          <hr class="whitespace" style="height: 35vh;" />
        </div>
        <p>The resulting trees all look reasonably different and also have single-home leaf nodes.</p>
        <p>These seemingly-esoteric homes that may result in single-data-point leaf nodes are actually a normal part of any data set. They are an outcome of the <em>method</em> for fitting the model.  </p>
        <p>
          When the <em>minimum node size</em> parameter is one, the tree grows until every branch has a homogeneous leaf node.</p>
        
        <p>For a given data set, growing the tree on a different set of homes changes <em>what</em> the branches overfit to, but overfitting still occurs. 
          <span class="footnote-anchor-v2">
            <span> 
              The data can also be a source of error. An example of <em>data</em> resulting in a biased model is <a href="https://dism.ssri.duke.edu/survey-help/tipsheets/tipsheet-nonresponse-error" target= "_blank">non-response</a> in polling. 
            </span>
          </span>
          Models that overfit are unstable and sensitive to small changes in the training data and thus high variance.
        </p>
        <hr class="whitespace" style="height: 35vh;" />

        <div id="increase-minimal-node-size">
          <h2>The Bias-Variance Tradeoff</h2>
          <p>One way to address errors from overfitting is to impose limits on how a tree grows by changing the minimum-node-size threshold.</p>
          <hr class="whitespace" style="height: 25vh;" />
          <p>As the minimum-node-size threshold increases, there are fewer splits. The trees get less <em>bushy</em>. </p>
          <div id="pause-minimal-node-size">
            <hr class="whitespace" style="height: 30vh;" />
            <p>The accuracy of the each tree improves as errors due to variance decrease.<p>
            <hr class="whitespace" style="height: 15vh;" />
          </div>
          <hr class="whitespace" style="height: 20vh;" />
          <p>As the minimum-node-size threshold continues to increase, the accuracy begins to deteriorate from error due to bias.</p>
          <hr class="whitespace" style="height: 35vh;" />
          <p>Until you get back to a stump.</p>
        </div>
        <hr class="whitespace" style="height: 35vh;" />

        <p> A model that is overly-simplistic is just as problematic as one that is overly-scrupulous. Errors due to bias and those due to variance are distinct. 
          Understanding the tradeoff between bias and variance (and how different model types let you balance the two) is foundational to modeling well.</p>  
        <hr class="whitespace" style="height: 20vh;" />
      </div>
    </div>
  </div>
  <hr class="whitespace" style="height: 10vh;" />

  <div class="row" id="abstracted-trade-off" style="clear: both;">
    <div class="col-xs-5">
      <hr class="whitespace" style="height: 10vh;" />
      <h2>The Trade-off in Abstract Terms</h2>
      <p>A common way to visualize the relationship between model complexity and accuracy is on a chart.
      </p>
      <p>The relationship between a parameter like minimum node size and model error  
        illustrates the tradeoff between bias and variance more explicitly.</p>
      <hr class="whitespace" style="height: 20vh;" />
      <div id="bias-decreases">
        <hr class="whitespace" style="height: 20vh;" />
        <p>When a model is less complex, it ignores relevant information, and error due to bias is high. As the model becomes more complex, error due to bias decreases.</p>
        <hr class="whitespace" style="height: 20vh;" />
      </div>
      <div id="variance-increases">
        <hr class="whitespace" style="height: 20vh;" />
        <p> On the other hand, when a model is less complex, error due to variance is low. Error due to variance increases as complexity increases. </p>        
        <hr class="whitespace" style="height: 20vh;" />
      </div>
      <div id="balance-minimum-error">
        <hr class="whitespace" style="height: 20vh;" />
        <p>Overall model error is a function of error due to bias plus error due to variance. The ideal model minimizes error from each.</p>
        <p>You can actually show mathematically that error due to bias and error due to variance are distinct.
            <span class="footnote-anchor-v2">
              <span> 
                Necessary caveats: This excludes irreducible error (the variance of error terms). Also, "Bias" is actually "Bias<sup>2</sup>." 

              </span>
            </span>
        </p>
      </div>          
      <hr class="whitespace" style="height: 30vh;" />
      <h2>Final Thoughts</h2>
      <p>Even at their optimal depth, single decision trees aren’t the best performing models. While trees are very easy to understand, the world is more complex than a bunch of if-then statements. </p>     
      <p>Nevertheless, decision trees can be used in aggregate to yield very strong results. We'll discuss these <strong>ensemble methods</strong> in Part III. </p>
      <hr class="whitespace" style="height: 32vh;" />
    </div>
  </div>
	<div class="row">
    <div class="col-xs-8 col-xs-offset-2">
      <h2>Recap</h2>
      <ol>
        <li>Models approximate real-life situations using limited data.</li>
        <li>In doing so, errors can arise due to assumptions that are overly simple (bias) or overly complex (variance).</li>
        <li>Building models is about making sure there's a balance between the two.</li>
      </ol>
      <hr class="whitespace" style="height: 25vh;" />
      <p>Questions? Thoughts? We would love to hear from you. Tweet us at <a href="https://twitter.com/r2d3us" target="_blank">@r2d3us</a> or email us at <a href="mailto:team@r2d3.us">team@r2d3.us</a>.</p>
      <p>Want to be notified when the next post is released?</p>
      <hr class="whitespace" style="height: 5vh;" />
    </div>

		<div>



<div id="resolution-disclaimer">
  <div id="disclaimer-wrapper">
    <p>We are so sorry, but we designed this site for desktop
    rather than mobile viewing. Please come back on
    a desktop (or a screen at least 1024 by 768 pixels in size)!</p>
    <p id="dismiss-disclaimer">&hellip;or go ahead anyway.</p>
  </div>
</div>
<script type="text/javascript" src="/static/dist/c814f45e5cce5d96ddf550688fefd942.js"></script>
<script src="/static/pages/decision-trees-part-1/housing-data/tree-training-set-98.js"></script>
<script src="/static/pages/decision-trees-part2-v2/bundle.js"></script>
<script>
  const lang = document.getElementsByTagName('html')[0].getAttribute('lang')
  window.lang = lang || "en"
</script>

<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = 'https://connect.facebook.net/en_US/sdk.js#xfbml=1&version=v3.0';
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>


  </body>
</html>
